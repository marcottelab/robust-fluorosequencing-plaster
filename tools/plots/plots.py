"""
Plaster-specific plots

These fall into two categories:
    * Mature: plots that are ready to be used in notebook report templates
    * Development: Plots that are still being worked on across various notebooks

Note:
    * All plots are free-functions
    * All plots should accept a run parameters and *optional* modifications to
      the default plot.
    * Naming conventions:
        def text_* dump information as print statements
        def plot_* are plots
        def wizard_* are interactive wizard-type components that may have plot components
    * bokeh imports should be deferred (they tend to slow down tests)
"""

import logging

import numpy as np
import pandas as pd
import structlog

from plaster.run.lnfit.lnfit_result import LNFitResult
from plaster.tools.ipynb_helpers.displays import hd, md
from plaster.tools.schema import check
from plaster.tools.utils import utils
from plaster.tools.zplots.zplots import ZPlots

log = logging.getLogger(__name__)

logger = structlog.get_logger()

# Mature
# ====================================================================================================

# On some of our plots, e.g. peptide PR, if there are too many traces it causes the browser
# to really chug, even when just scrolling the page because something is getting updated/redrawn.
# Short of figuring out how to do this better, many fns avoid too many traces, so define a
# constant that can be used in multiple places.
MAX_BOKEH_PLOT_TRACES = 50

# Prep & sim related
# -----------------------


def text_prep_and_sim_info(run):
    """
    State statistics about labelling and sim including
    protein identifiability under the labelling scheme.

    Audience:
        Basic users

    Goal:
        Allow the user to understand the experimental setup
        and the high-level overview of the label space.
    """
    if hasattr(run, "sim_v1"):
        n_train_non_zero_recall_peps = (run.sim_v1.train_pep_recalls > 0.0).sum()
        n_train_zero_recall_peps = (run.sim_v1.train_pep_recalls == 0.0).sum()
        n_train_peps = n_train_non_zero_recall_peps + n_train_zero_recall_peps

        n_test_non_zero_recall_peps = (run.sim_v1.test_recalls > 0.0).sum()
        n_test_zero_recall_peps = (run.sim_v1.test_recalls == 0.0).sum()
        n_test_peps = n_test_non_zero_recall_peps + n_test_zero_recall_peps
        print(
            f"The preparation consisted of:\n"
            f"  Proteins: {len(run.prep.pros()) - 1}.\n"
            f"    Of which: {len(run.prep.pros__from_decoys())} are decoys generated by the {run.prep.params.decoy_mode} method.\n"
            f"  Labels: {run.sim_v1.params.to_label_str()}.\n"
            f"  Protease: {run.prep.params.protease} including {run.prep.params.include_misses} missed cleavages.\n"
            f"  Peptides: {len(run.prep.peps()) - 1}.\n"
            f"    Unique: {len(run.prep.pepstrs().seqstr.drop_duplicates()) - 1}\n"
            f"    From real sources: {len(run.prep.peps__no_decoys()) - 1}\n"
            f"    From decoy sources: {len(run.prep.peps__from_decoys())}\n"
            f"Simulation was run with:\n"
            f"  n_pres: {run.sim_v1.params.n_pres}, n_mocks: {run.sim_v1.params.n_mocks}, n_edmans: {run.sim_v1.params.n_edmans}\n"
            f"  Train set:\n"
            f"    {n_train_non_zero_recall_peps} ({100 * n_train_non_zero_recall_peps / n_train_peps:2.0f}%) have positive recall (observable)\n"
            f"    {n_train_zero_recall_peps} ({100 * n_train_zero_recall_peps / n_train_peps:2.0f}%) had zero recall (unlabelable)\n"
            f"  Test set:\n"
            f"    {n_test_non_zero_recall_peps} ({100 * n_test_non_zero_recall_peps / n_test_peps:2.0f}%) have positive recall (observable)\n"
            f"    {n_test_zero_recall_peps} ({100 * n_test_zero_recall_peps / n_test_peps:2.0f}%) had zero recall (unlabelable)\n"
        )
    else:
        # TODO: find sim_v2 appropriate replacements for sim_v1 params listed above
        print(
            f"The preparation consisted of:\n"
            f"  Proteins: {len(run.prep.pros()) - 1}.\n"
            f"    Of which: {len(run.prep.pros__from_decoys())} are decoys generated by the {run.prep.params.decoy_mode} method.\n"
            f"  Protease: {run.prep.params.protease} including {run.prep.params.include_misses} missed cleavages.\n"
            f"  Peptides: {len(run.prep.peps()) - 1}.\n"
            f"    Unique: {len(run.prep.pepstrs().seqstr.drop_duplicates()) - 1}\n"
            f"    From real sources: {len(run.prep.peps__no_decoys()) - 1}\n"
            f"    From decoy sources: {len(run.prep.peps__from_decoys())}\n"
            f"Simulation was run with sim_v2.\n"
        )
    # TODO: if the run has a "protein of interest" then repeat a lot of the above for
    # just the protein of interest -- e.g. how many peptides, unique, positive recall, etc.
    if (
        run.prep.n_pois > 0 and run.prep.n_pros > 2
    ):  # ==2 means 1 protein and the 'null' entry
        pro_ids = run.prep.pros__pois().pro_id
        pep_iz = run.prep.peps__pois().pep_i.unique()
        pepstrs = run.prep.pepstrs()
        pepstrs = pepstrs[pepstrs.pep_i.isin(pep_iz)]

        poi_string = f"{list(pro_ids)}" if len(pro_ids) < 10 else f"{len(pro_ids)}"
        print(
            f"\n\n"
            f"Protein(s) of interest: {poi_string}\n"
            f"  Peptides: {len(pep_iz)}\n"
            f"    Unique: {len(pepstrs.seqstr.drop_duplicates())}\n"
            f"  TODO: more stats on just this protein(s)\n"
        )


# Classification related
# -----------------------


def text_call_score_info(run, classifier=None):
    """
    State statistics about classification.

    Audience:
        Advanced users

    Goal:
        Allow the user to see:
            How many uniq FluoroSeqs, PeptideSeqs, and ProteinSeqs were called true and pred.
            How many real vs decoys were called with counts

    classifier: None to use any available preferred classifier, or one of the
                supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'

    """

    bag = run.test_call_bag(classifier=classifier)
    n_zeros = (bag.pred_pep_iz == 0).sum()

    pred = bag.pred_peps__pros()
    n_pred_real = (pred.pro_is_decoy < 1).sum()
    n_pred_decoy = (pred.pro_is_decoy > 0).sum()
    n_preds = len(pred)

    print(
        f"Test {bag.classifier_name.upper()} classification result has {bag.n_rows} rows.\n"
        f"  Of which: {n_zeros} were predicted to empty.\n"
        f"True calls came from:\n"
        f"  {len(bag.true_peps__pros())} samples over {bag.true_peps__pros().groupby('pep_i').ngroups} total peptides of which {len(bag.true_peps__unique())} were unique\n"
        f"Predicted calls went to:\n"
        f"  {len(bag.pred_peps__pros())} samples covering {len(bag.pred_peps__unique())} unique peptides\n"
        f"  {100.0 * n_pred_real / n_preds:2.0f}% reals and {100.0 * n_pred_decoy / n_preds:2.0f}% decoys\n"
    )


def pep_iz_in_report(run, with_ptms=False):
    if with_ptms:
        return run.prep.peps__ptms(poi_only=True, ptms_to_rows=False).pep_i.unique()
    return run.prep.peps__pois().pep_i.values


def plot_peptide_effective_labelability(run, **kwargs):
    """
    'effective labelability' means how often does a peptide result in a dye-track signature that is
    non-zero.  A 'zero' signature might occur because the peptide detaches immediately, a dye
    is a dud, a dye bleaches immediately (in a mock cycle), or, most typically, that the peptide
    has no amino acids that can be labeled.  The latter is 'unlabelable'.  All the others are
    'effectively unlabelable'

    I am using the name 'labelability' because the next fn in this file plots observability vs
    precision, and it is confusing to use the word observability for both plots.  I have also
    considered "scope_observability" vs "classifier_observability".  The confusion arises when
    you look at the plot produced by this function, which looks only at "train recalls", and see
    that 100%, or nearly 100%, of peptides are "observable".  And then in the next set of plots
    which look at PR for peptides, you see that only 80% of the peptides are observable even at
    precision=0.  How can this be?  It is because even if a peptide has labels and produces a
    non-zero dye-track (so it is "scope observable" or "labelable"), it may NEVER get picked
    by the classifier because some other peptide class receives a higher score.  So in our
    "post classification" notion of PR, even at precision 0, the peptide is not observable
    by the classifier.

    Y axis is the fraction 'scope' observable, or 'effectively labelable'
    X axis is rank ordered peptides where peptide with highest “frac observable” is on the left.
    Some of the peptides will have zero observability, we call those by the special name “unlabelable”

    Up to three plots may be displayed:
    1. All peptides the classifier will be trained on
    2. Only peptides belonging to any "protein_of_interest"; see set_protein_of_interest()
    3. Only peptides satisfying (2) and additionally containing PTM locations

    Goal:
        Allow user to see:
            What portion/how well the peptides can be "seen" given the current labelling scheme.
    """

    pep_iz_poi = pep_iz_in_report(run, with_ptms=False)
    pep_iz_ptm = pep_iz_in_report(run, with_ptms=True)

    pep_indices = [np.array(range(1, run.sim_v2.train_pep_recalls.shape[0]))]
    if len(pep_iz_poi):
        pep_indices.append(np.sort(pep_iz_poi))
    if len(pep_iz_ptm):
        pep_indices.append(np.sort(pep_iz_ptm))

    z = ZPlots.zplot_singleton
    with z(
        _cols=len(pep_indices),
        f_y_axis_label="per-class fraction labelable",
        f_x_axis_label="peptide class rank",
        f_y_range=(0, 1),
        **kwargs,
    ):

        for idx, domain in zip(pep_indices, ["", "POI ", "PTM "]):
            train_pep_recalls = run.sim_v2.train_pep_recalls[idx]

            train_pep_recalls_sorted = np.sort(train_pep_recalls)[::-1]
            n_classes = len(train_pep_recalls_sorted)
            n_observable = np.sum(train_pep_recalls_sorted.astype(bool))
            z.cols(
                train_pep_recalls_sorted,
                f_title=f"{domain}{n_observable} of {n_classes} ({100.0 * n_observable / n_classes:,.1f}%) peptides labelable",
            )


def plot_peptides_per_fluorosequence(run, **kwargs):
    peps__flus = (
        run.sim_v2.peps__flus(run.prep)
        .drop_duplicates("flustr")
        .sort_values("flu_count", ascending=False)
        .reset_index()
    )
    labels = peps__flus.apply(lambda x: f"{x.flustr} ({x.flu_count})", axis=1)
    z = kwargs.pop("_zplot_context", ZPlots())
    # Note 1: below -- we are not showing the unlabeled peptides.
    z.cols(
        peps__flus.flu_count.values[1:],
        _label=labels.values[1:],
        _size_x=1000,
        f_title="(Labelable) Peptides per fluorosequence",
        f_x_axis_label="fluorsequence class rank",
        f_y_axis_label="peptide count",
        **kwargs,
    )


def _plot_peptide_observability_vs_precision(
    pr_df, pep_iz=None, as_fraction_observable=True, pr_axes=True, **kwargs
):
    """
    given a precision-recall information for a set of peptides, plot
    the fraction of those pep_i observable as a function of precision.

    pr_df:                  a df that contains precision,recall values for each pep_i
    pep_iz:                 the subset of pep_i to consider, or None or all pep_i
    as_fraction_observable: when True show observability as fraction of total peptides
    pr_axes:                when True orient the plot as standard PR plots, with PEPTIDE
                            recall on the x-axis.

    Y axis is number of classes or fraction of classes observable at precision X
    X axis is precision

    Goal:
        Answer question:
            How many of the peptide classes can I observe at precision X?
            (or, equivalently)
            At precision X, how many peptide classes have a recall > 0?
    """

    pep_iz_with_pr = list(pr_df.pep_i.unique())

    #     no_pr = [pi  for pi in pep_iz if pi not in pep_iz_with_pr ]
    #     print(no_pr)

    assert pep_iz is None or all(
        [pi in pep_iz_with_pr for pi in pep_iz]
    ), "pr_df must contain PR info for all pep_iz"

    if pep_iz is None:
        pep_iz = pep_iz_with_pr

    pr_by_pep = pr_df[pr_df.pep_i.isin(pep_iz)]

    n_peps_at_precision = []
    precisions = np.linspace(0, 1, 51)
    for prec in precisions:
        peps_observable_at_prec = pr_by_pep[
            (pr_by_pep.prec >= prec) & (pr_by_pep.recall > 0)
        ].pep_i.unique()
        n_peps_at_precision += [len(peps_observable_at_prec)]

    n_classes = len(pep_iz)

    y_range = (0, n_classes)
    if as_fraction_observable:
        n_peps_at_precision = np.array(n_peps_at_precision) / n_classes
        y_range = (0, 1.05)
    x_range = (0, 1.05)

    y_axis = (
        "fraction of classes observable"
        if as_fraction_observable
        else "peptide classes observable"
    )
    x_axis = "precision"

    y_values = n_peps_at_precision
    x_values = precisions

    if not pr_axes:  # because this was the default for a long time
        args = dict(
            x=x_values,
            y=y_values,
            f_x_axis_label=x_axis,
            f_y_axis_label=y_axis,
            f_x_range=x_range,
            f_y_range=y_range,
        )
    else:
        args = dict(
            y=x_values,
            x=y_values,
            f_y_axis_label=x_axis,
            f_x_axis_label="peptide-classes recall",
            f_y_range=x_range,
            f_x_range=y_range,
        )

    z = ZPlots.zplot_singleton
    z.line(**args, **kwargs, line_width=2)


def plot_peptide_observability_vs_precision(
    run,
    pep_iz=None,
    pep_subset="all",
    as_fraction_observable=True,
    pr_axes=True,
    classifier=None,
    **kwargs,
):
    """
    See docs at _plot_peptide_observability_vs_precision()

    pep_iz:     list of peptide indices that should be plotted, or None.  May
                be used together with pep_subset.

    pep_subset: named subset controlling which portion of peptides the test_call_bag
                that should be plotted.  Valid subset names are all, poi, and ptm.

    pr_style_axes: If True, orient the axes as a standard PR plot, with PEPTIDE
                   recall on the x-axis.  Otherwise, interpret as
                   "peptide observability vs precision"

    classifier: None to use any available preferred classifier, or one of the
                supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'
    """
    bag = run.test_call_bag(classifier=classifier)
    pr_by_pep_all = bag.pr_curve_by_pep()

    domain = ""
    if pep_subset == "all":
        pep_iz_subset = list(pr_by_pep_all.pep_i.unique())
    elif pep_subset == "poi":
        pep_iz_subset = pep_iz_in_report(run, with_ptms=False)
        domain = "POI "
    elif pep_subset == "ptm":
        pep_iz_subset = pep_iz_in_report(run, with_ptms=True)
        domain = "PTM "
    else:
        raise ValueError("pep_subset must be all, poi, or ptm")

    if pep_iz is not None and len(pep_iz) > 0:
        pep_iz_subset = [pi for pi in pep_iz_subset if pi in pep_iz]

    z = ZPlots.zplot_singleton
    with z():
        vs_text = (
            "Peptide-Classes PR" if pr_axes else "Peptide Observability vs Precision"
        )
        title = f"{bag.classifier_name.upper()} {domain}{vs_text}, {len(pep_iz_subset)} classes"
        _plot_peptide_observability_vs_precision(
            pr_by_pep_all,
            pep_iz_subset,
            as_fraction_observable=as_fraction_observable,
            pr_axes=pr_axes,
            f_title=title,
            **kwargs,
        )


def plot_call_score_hist(run, classifier=None, **kwargs):
    """
    Demonstrate an overview of classification scores for predictions to non-decoy versus decoys.

    Audience:
        Advanced users

    Goal:
        Allow the user to see:
            if the decoy and real false rates have roughly equal distributions
            if high scores are a good predictor of correctness.

    Plan:
        This is probably not a long-term useful plot as the imposter map
        will probably communicate most of this information in a more useful way.

    classifier: None to use any available preferred classifier, or one of the
                supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'
    """

    bag = run.test_call_bag(classifier=classifier)

    z = ZPlots.zplot_singleton
    with z(
        _bins=np.linspace(0, 1, 20),
        _merge=True,
        line_alpha=1,
        f_width=500,
        f_height=250,
        f_x_range=(0, 1),
        f_y_range=(0, 0.9),
        f_x_axis_label="Score",
        f_y_axis_label="Count",
        f_title=f"Normalized distribution of call scores - {bag.classifier_name.upper()}",
        _step=True,
        _legend_click_policy="hide",
    ):

        correct_mask = bag.true_pep_iz == bag.pred_pep_iz
        real_mask = bag.pred_peps__pros().pro_is_decoy < 1
        decoy_mask = bag.pred_peps__pros().pro_is_decoy > 0
        mask = correct_mask & real_mask

        if np.any(mask):
            z.hist(
                bag.scores[mask],
                _normalizer=mask.sum(),
                line_color=ZPlots.feature,
                line_width=3,
                legend_label="Correct calls predicted to an any-real source",
            )

        wrong_mask = ~correct_mask
        mask = wrong_mask & real_mask
        if np.any(mask):
            z.hist(
                bag.scores[mask],
                _normalizer=mask.sum(),
                line_color=ZPlots.compare1,
                line_width=2,
                line_dash=[2, 2],
                legend_label="Wrong calls predicted to an any-real source",
            )

        if np.any(decoy_mask):
            z.hist(
                bag.scores[mask],
                _normalizer=decoy_mask.sum(),
                line_color=ZPlots.compare2,
                line_width=2,
                line_dash=[5, 3],
                legend_label="Any calls predicted to an any-decoy source",
            )


def plot_pr_curve(prsa, **kwargs):
    """
    Plot one pr curve.

    Note that the pr_curve might have a very large number of points
    and this makes the plotting slow. The information at the top (high prec)
    is usually more interesting so we sample from the set logarithmically.

    Arguments:
        prsa: a 4-tuple of precision, recall, min_score, AUC generated by AssignmentEval.pr_curve or similar.

    TASK
        I want this to have hover tools but I need to sort out hovers using zplots.
        But I also need this to be zplot mergable so for now I'm taking out the hover.
    """

    check.affirm(len(prsa) == 4)
    check.list_or_tuple_t(prsa[0:2], np.ndarray)
    n_rows = prsa[0].shape[0]
    check.array_t(prsa[0], (n_rows,))
    check.array_t(prsa[1], (n_rows,))

    n_prs = len(prsa[0])
    if n_prs <= 1:
        logger.error("Need minimum 1 n_prs", n_prs=n_prs)
        return

    kwargs = utils.set_defaults(kwargs, line_width=2, line_alpha=1.0, _noise=0.000)

    # Add noise to keep them from sitting on top of each other
    # An _noise of about 0.005 is typically sufficient if any is needed
    _noise = kwargs.pop("_noise")
    x_noise = np.random.uniform(-_noise, +_noise, size=n_rows)
    y_noise = np.random.uniform(-_noise, +_noise, size=n_rows)

    z = ZPlots.zplot_singleton
    z.line(
        x=prsa[1] + x_noise,
        y=prsa[0] + y_noise,
        _range=(0, 1.05, 0, 1.05),
        f_x_axis_label="read recall",
        f_y_axis_label="precision",
        f_title=kwargs.pop("f_title", "Precision-Recall"),
        **kwargs,
    )


def plot_pr_aggregate(run, pep_iz=None, classifier=None, **kwargs):
    """
    Show P/R for all some set aggregate set of peptides
    (ie. renders a single line for the set. See also: plot_pr_breakout)

    Arguments:
        pep_iz: If None computes over all peps, otherwise the subset
        classifier: None to use any available preferred classifier, or one of the
                    supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'
        kwargs: Passed to the zplot

    Audience:
        Trained users.
        P/R curves will not be familiar to many customers but it is an important concept to teach
        as it is central to the way we treat evidence.

    Goal:
        Allow user to see:
            The P/R curve for ALL peptides as an aggregate statistic

    Plan:
        There's a lot of way to look at this and it probably needs
        to be subdivided into separate curves or made into an interactive widget.

        In general, I'm going to avoid having lots of switches on this
        and instead rely on with z.Opts(_merge=True) when the need to be overlaid.
    """
    cb = run.test_call_bag(classifier=classifier)
    prsa = cb.pr_curve_pep(pep_iz_subset=pep_iz)
    utils.set_defaults(
        kwargs, f_title=f"{cb.classifier_name.upper()} P/R over all peptides"
    )
    plot_pr_curve(prsa, **kwargs)


def plot_pr_breakout(run, pep_iz=None, classifier=None, **kwargs):
    """
    Render a separate P/R curve for each pep_iz.
    See plot_pr_aggregate()

    Arguments:
        If pep_iz is None it renders ALL peptides. (Might be large!)
        classifier: None to use any available preferred classifier, or one of the
                    supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'

        kwargs: passed to zplot
    """
    master_bag = run.test_call_bag(classifier=classifier)

    if pep_iz is None:
        pep_iz = list(range(1, run.prep.n_peps))

    utils.set_defaults(
        kwargs, f_title=f"{master_bag.classifier_name.upper()} P/R over all peptides"
    )
    z = ZPlots.zplot_singleton
    user_color = kwargs.get("color", None)
    with z(
        _merge=True,
        _range=(0, 1.05, 0, 1.05),
        f_x_axis_label="read recall",
        f_y_axis_label="precision",
        **kwargs,
    ):
        pr_df = master_bag.pr_curve_by_pep(pep_iz=pep_iz)
        peps_flus_df = master_bag.peps__pepstrs__flustrs__p2()
        for pep_i, g in pr_df.groupby("pep_i"):
            prsa = (g.prec.values, g.recall.values, g.score.values, [])
            # for pep_i in pep_iz:
            #     prsa = master_bag.pr_curve_by_pep(pep_iz_subset=[pep_i])

            row = peps_flus_df[peps_flus_df.pep_i == pep_i].iloc[0]

            legend_label = f"pep{pep_i:03d}"
            line_label = f"{row.pro_id} pep{pep_i:03d} {row.seqstr} {row.flustr}"

            plot_pr_curve(
                prsa,
                line_color=user_color or z.next(),
                _label=line_label,
                legend_label=legend_label,
                **kwargs,
            )

            # this will only get plotted if all class scores are available:
            prsa = master_bag.pr_curve_sklearn(pep_i)
            if prsa[0] is not None:
                plot_pr_curve(prsa, line_width=1, **kwargs)


def plot_pr_for_run(
    run, aggregate_only=False, force_all_proteins=False, classifier=None, **kwargs
):
    """
    Display one or more PR plots depending on the config of run.

    Plot aggregate and breakout PRs for protein(s) of interest if any are set,
    otherwise plot aggregate of all proteins/peptides.

    Note that individual PR curves are not plotted if the number of peptides exceeds
    some value (currently 50) because of some slowdown in notebooks when there are
    lots of individual curves.  This slowdown needs to be investigated - it should be
    able to do hundreds without any issue.  Right?

    aggregate_only: Do not plot PR curves for any individual peptides, whether the domain
                    is all proteins, or proteins of interest, or proteins with PTMs.

    force_all_proteins: Plot PR for *all* proteins (always an aggregate PR) even if there
                        are proteins-of-interest specified for this run.

    classifier: None to use any available preferred classifier, or one of the
                supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'

    kwargs: passed to zplot

    NOTE: in my experience, aggregate PR curves are not very informative.
    """

    z = ZPlots.zplot_singleton
    zOpts = (
        {}
        if (
            aggregate_only
            or run.prep.n_pois == 0
            or run.prep.n_pois > MAX_BOKEH_PLOT_TRACES
        )
        else {"_cols": 3}
    )

    with z(**zOpts):
        z.color_reset()

        # *Up to* three columns, if we have interesting domains to look at.
        if run.prep.n_pois > 0 and not force_all_proteins:
            pep_iz = run.prep.peps__pois().pep_i.unique()
            n_peps = len(pep_iz)
            if n_peps > 1 or aggregate_only:
                plot_pr_aggregate(
                    run,
                    pep_iz=pep_iz,
                    classifier=classifier,
                    f_title=f"{classifier.upper()} Aggregate PR ({n_peps} peptides of interest)",
                    **kwargs,
                )
            # maybe do breakout of individual peps of interest
            if not aggregate_only:
                if n_peps < MAX_BOKEH_PLOT_TRACES:  # See comment at top.
                    plot_pr_breakout(
                        run,
                        pep_iz=pep_iz,
                        classifier=classifier,
                        f_title=f"{classifier.upper()} Individual PR ({n_peps} peptides of interest)",
                        **kwargs,
                    )
                else:
                    logger.warning(
                        "Individual peptide PR for protein-of-interest skipped because n_peps is large",
                        n_peps=n_peps,
                    )

            # maybe do breakout of peps of interest that have PTM locations
            df = run.prep.peps__ptms(poi_only=True, ptms_to_rows=False)
            pep_w_ptm_iz = df.pep_i.values
            n_peps_w_ptm = len(pep_w_ptm_iz)
            if n_peps_w_ptm > 0 and not aggregate_only and n_peps_w_ptm != n_peps:
                if n_peps_w_ptm < MAX_BOKEH_PLOT_TRACES:  # See comment at top.
                    plot_pr_breakout(
                        run,
                        pep_iz=pep_w_ptm_iz,
                        classifier=classifier,
                        f_title=f"{classifier.upper()} Individual PR ({n_peps_w_ptm} peptides w PTMs)",
                        **kwargs,
                    )
                else:
                    logger.warning(
                        "Individual PTM-peptide PR skipped because n_peps_w_ptm is high",
                        n_peps_w_ptm=n_peps_w_ptm,
                    )

        else:
            # All proteins, less the "null" entry at 0
            n_peps = run.prep.n_peps - 1
            n_pros = run.prep.n_pros - 1
            plot_pr_aggregate(
                run,
                classifier=classifier,
                f_title=f"{classifier.upper()} Aggregate PR ({n_pros} proteins, {n_peps} peptides)",
                **kwargs,
            )


def plot_pr_for_job(job, force_all_proteins=False, classifier="rf_v2", **kwargs):
    """
    Single plot containing relevant PR per run, with legend to indicate run.

    force_all_proteins: Plot PR for *all* proteins (aggregate) even if there
                        are proteins-of-interest specified for this run.

    classifier: None to use any available preferred classifier, or one of the
                supported classifiers in RunResult::test_call_bag(), e.g. 'rf', 'nn'

    kwargs: passed to zplot
    """

    domain = f"all proteins, observable peptides per run"

    run = job.runs[0]  # get proteins_of_interest, same for each run
    if run.prep.n_pois > 0 and not force_all_proteins:
        domain = (
            f"{run.prep.n_pois} protein(s) of interest, observable peptides per run"
        )

    from bokeh.palettes import Category20

    palette = Category20[20]

    z = ZPlots()
    with z(
        _merge=True,
        f_title=f"PR all runs, {domain}",
        f_x_axis_label="read recall",
        f_y_axis_label="precision",
    ):
        for i, run in enumerate(job.runs):
            color = palette[i % 20]
            name = "_".join(run.run_name.split("_")[:-1])
            plot_pr_for_run(
                run,
                aggregate_only=True,
                force_all_proteins=force_all_proteins,
                classifier=classifier,
                color=color,
                legend_label=name,
                **kwargs,
            )


def standard_run_report(run, display_run_title=True, classifier=None):

    if display_run_title:
        hd("h2", f"Run: {run.run_name}")

    # Information from Prep and Simulation that has nothing
    # to do with classification
    hd("h3", f"Prep and simulation")
    text_prep_and_sim_info(run)
    if hasattr(run, "sim_v2"):
        plot_peptide_effective_labelability(run)
        plot_peptides_per_fluorosequence(run)

    # If no classifier is specified, print information from all available
    # classifiers, otherwise just the one that was requested.
    hd("h3", f"Classification")
    classifiers = run.get_available_classifiers()
    if classifier in classifiers:
        classifiers = [classifier]

    # Do textual information on classifiers one after the other.
    for classifier in classifiers:
        text_call_score_info(run, classifier=classifier)

    z = ZPlots()

    # Do call/score histograms next to each other
    with z(_cols=len(classifiers)):
        for classifier in classifiers:
            plot_call_score_hist(run, classifier=classifier)

    # Do PR plots for each classifier grouped together. If there are proteins of
    # interest, then each call will already result in multiple plots, but if there are
    # not, each call will only be one plot, so in that case group them on the same
    # to to make comparison between classifiers easier.
    zOpts = {}
    if run.prep.n_pois == 0 or run.prep.n_pois > MAX_BOKEH_PLOT_TRACES:
        zOpts = {"_cols": 2}
    with z(**zOpts):
        for classifier in classifiers:
            plot_peptide_observability_vs_precision(run, classifier=classifier)
            plot_pr_for_run(run, _noise=0.01, classifier=classifier)


# In development
# ====================================================================================================


def plot_channel_signal_scatter(run, ch0=0, ch1=1, **kwargs):
    sample_size = kwargs.get("sample_size", 500)
    n_std_plot_range = kwargs.get("n_std_plot_range", 4)

    sigproc = run.sigproc_v1 if run.has_result("sigproc_v1") else run.sigproc_v2
    r = sigproc.radmats()
    n_peaks = len(r.peak_i.unique())

    title = (
        f"ch{ch1} vs ch{ch0} signal, n={sample_size}, axis-range={n_std_plot_range}-STD"
    )
    md(f"## {title}")

    sample = np.random.choice(range(n_peaks), replace=False, size=sample_size)

    max_x = max_y = 0
    z = ZPlots()
    with z(_cols=4, f_width=250, f_height=280):
        for cy in range(sigproc.n_cycles):
            x = r[(r.cycle_i == cy) & (r.channel_i == ch0)].signal.values
            y = r[(r.cycle_i == cy) & (r.channel_i == ch1)].signal.values
            x_sample = x[sample]
            y_sample = y[sample]
            if max_x == 0:
                max_x = np.mean(x_sample) + np.std(x_sample) * n_std_plot_range
                max_y = np.mean(y_sample) + np.std(y_sample) * n_std_plot_range
            z.scat(
                x=x_sample,
                y=y_sample,
                f_x_axis_label=f"signal, channel {ch0}",
                f_y_axis_label=f"signal, channel {ch1}",
                f_title=f"cycle {cy}",
                f_x_range=(
                    1
                    if kwargs.get("f_x_axis_type") == "log"
                    else 0,  # Bokeh has issues with zeros in log scale plots https://github.com/bokeh/bokeh/issues/6536
                    max_x,
                ),
                f_y_range=(0, max_y),
                **kwargs,
            )


def text_lnfit_links(run):
    from plumbum import local

    names = LNFitResult.task_names(run)
    logger.info("lnfit tasks in run", names=names)

    # We can use any name to get an lnfit result object
    # which deals with multiple lnfits per run for us.

    lnfit = run[names[0]]

    cwd = local.cwd
    for ch, html_files in enumerate(lnfit.html_files()):
        if not html_files:
            # TODO: should we print some kind of warning here? Due to the structure of the data returned by LNFitResult.html_files,
            #       if html_files is empty then there's not a whole lot of other information to relay to the user.
            continue

        task_folder = html_files[0].split()[-2]
        md(f"### {task_folder}")
        m = ""
        for f in sorted(html_files):
            relative_path = f.relative_to(cwd)
            m += f"* [{f.name}]({relative_path})\n"
        md(m)


def plot_signal_for_lnfit_sequence(run, channel, sequence, lnfit_taskname=None):
    # Used in lnfit_template
    """
    Plot signal vs cycle for all peaks with a given lnfit sequence
    on a particular channel.

    sequence is e.g. '22221111000000'
    """

    names = LNFitResult.task_names(run)
    lnfit = run[names[0]]

    df_ln = lnfit.lnfit_bestseq_df(
        lnfit_taskname
    )  # contains lnfit bestseq per peak_i and channel;

    sigproc = run.sigproc_v1 if run.has_result("sigproc_v1") else run.sigproc_v2
    df = sigproc.fields__n_peaks__peaks__radmat()
    df = pd.merge(df_ln, df, how="left", on=["peak_i", "channel_i"])

    filtered = df[(df.best_seq == sequence) & (df.channel_i == channel)].reset_index()
    peptide_count = int(len(filtered) / len(df.cycle_i.unique()))
    lnfit_name = lnfit_taskname or "All lnfits"
    md(
        f"### {peptide_count} peptides with lnfit sequence {sequence} on channel {channel}, {lnfit_name}"
    )

    if peptide_count > 0:
        # wizard_scat_df(filtered, default_x="cycle_i")
        # this now takes a run - todo?
        wizard_boxplot_df(
            filtered,
            "signal",
            "cycle_i",
            f_x_axis_label="Cycle",
            f_y_axis_label="Signal",
        )

    return filtered


def wizard_boxplot_df(
    df, value_col, group_col, f_x_axis_label=None, f_y_axis_label=None, f_title=None
):
    from bokeh.io import show  # Defer slow imports
    from bokeh.plotting import figure  # Defer slow imports

    #
    # Adapted from https://bokeh.pydata.org/en/latest/docs/gallery/boxplot.html
    #
    # This similar to the scat wizard, but with box & whisker.
    # find the quartiles and IQR for each category
    groups = df[[group_col, value_col]].groupby(group_col)
    q1 = groups.quantile(q=0.25)
    q2 = groups.quantile(q=0.5)
    q3 = groups.quantile(q=0.75)
    iqr = q3 - q1
    upper = q3 + 1.5 * iqr
    lower = q1 - 1.5 * iqr

    cats = sorted(list(groups.groups.keys()))

    # find the outliers for each category
    def outliers(group):
        cat = group.name
        return group[
            (group[value_col] > upper.loc[cat][value_col])
            | (group[value_col] < lower.loc[cat][value_col])
        ][value_col]

    out = groups.apply(outliers).dropna()

    # prepare outlier data for plotting, we need coordinates for every outlier.
    if not out.empty:
        outx = []
        outy = []
        for keys in out.index:
            outx.append(keys[0])
            outy.append(out.loc[keys[0]].loc[keys[1]])

    p = figure(
        tools="",
        background_fill_color="#efefef",
        title=f_title,
        x_axis_label=f_x_axis_label,
        y_axis_label=f_y_axis_label,
        toolbar_location=None,
    )

    # if no outliers, shrink lengths of stems to be no longer than the minimums or maximums
    qmin = groups.quantile(q=0.00)
    qmax = groups.quantile(q=1.00)
    upper[value_col] = [
        min([x, y]) for (x, y) in zip(list(qmax.loc[:, value_col]), upper[value_col])
    ]
    lower[value_col] = [
        max([x, y]) for (x, y) in zip(list(qmin.loc[:, value_col]), lower[value_col])
    ]

    # stems
    p.segment(cats, upper[value_col], cats, q3[value_col], line_color="black")
    p.segment(cats, lower[value_col], cats, q1[value_col], line_color="black")

    # boxes
    p.vbar(
        cats,
        0.7,
        q2[value_col],
        q3[value_col],
        fill_color="#E08E79",
        line_color="black",
    )
    p.vbar(
        cats,
        0.7,
        q1[value_col],
        q2[value_col],
        fill_color="#3B8686",
        line_color="black",
    )

    # whiskers (almost-0 height rects simpler than segments)
    p.rect(cats, lower[value_col], 0.2, 0.01, line_color="black")
    p.rect(cats, upper[value_col], 0.2, 0.01, line_color="black")
    # outliers
    if not out.empty:
        p.circle(outx, outy, size=6, color="#F38630", fill_alpha=0.6)

    p.xgrid.grid_line_color = None
    p.ygrid.grid_line_color = "white"
    p.grid.grid_line_width = 2
    p.xaxis.major_label_text_font_size = "12pt"

    show(p)


def abund_abund_plot(
    needle_iz,
    needle_impact,
    labels,
    ref_df,
    exp_names,
    dfs,
    n_classes,
    score_thresh=0.0,
    percent_mode=True,
    title="",
    x_label="",
    y_label="",
    highlight_impacted=True,
    omit_needle=False,
):
    from bokeh.models import PanTool  # Defer slow imports
    from bokeh.models import HoverTool, ResetTool, WheelZoomTool
    from bokeh.plotting import ColumnDataSource, figure, show  # Defer slow imports

    def abun(df, n_classes):
        return (
            df[~df.rejection_mask]
            .groupby("cls_pred")
            .size()
            .reset_index(name="calls")
            .set_index("cls_pred")
            .reindex(np.arange(n_classes), fill_value=0)
            .calls.values
        )

    abun = [abun(df[df.cls_score >= score_thresh], n_classes) for df in [ref_df] + dfs]

    # TASK? We could probably alter the abun() function above to groupby cls_pred and peak_i,
    # but in several attempts to do this I couldn't get it right.  So here I'll factor out
    # the number of cycles so that we don't return multiples of n_cycles. TFB
    abun = np.stack(abun).T // ref_df.groupby("peak_i").size()[0]

    b = np.sum(abun, axis=0)
    abun_percent = abun / np.tile(b, (abun.shape[0], 1))

    # abund_frame has the calls for each peptide class as rows and each experiment as columns
    abund_frame = pd.DataFrame(
        abun_percent if percent_mode else abun, columns=["ref"] + exp_names
    )

    assert len(exp_names) == len(dfs)
    n_exp = len(exp_names)

    # would like to scale vertex size by relative contribution to calls
    # vertex size should be ~ log(#calls) & in the range ~ [2,6]
    # I want fraction .001 to be about 2 and to double in size every order of mag.
    log_abun = abun_percent.copy()
    log_abun[log_abun < 0.0005] = 0.0005
    log_abun = (np.log(log_abun) + 8.0) * 2.0

    abund_log_scaled_frame = pd.DataFrame(log_abun, columns=["ref"] + exp_names)

    line_xs = []
    line_ys = []
    line_colors = []
    vertex_xs = []
    vertex_ys = []
    vertex_colors = []
    vertex_sizes = []
    vertex_labels = []

    first_vals = abund_frame["ref"].values

    needle_color = ZPlots.feature
    needle_impact_color = ZPlots.compare1
    other_color = ZPlots.compare2

    for cls_i in range(n_classes):
        is_needle = cls_i in needle_iz
        is_impacted = needle_impact[cls_i] != 0

        if omit_needle and (is_needle or is_impacted):
            continue

        color = (
            needle_color
            if is_needle
            else (
                needle_impact_color
                if (is_impacted and highlight_impacted)
                else other_color
            )
        )

        for exp_i in range(n_exp - 1):
            vals_exp_i0 = abund_frame[exp_names[exp_i + 0]].values
            vals_exp_i1 = abund_frame[exp_names[exp_i + 1]].values
            line_xs += [(vals_exp_i0[cls_i], vals_exp_i1[cls_i])]
            line_ys += [(first_vals[cls_i], first_vals[cls_i])]
            line_colors += [color]

        for exp_i in range(n_exp):
            vals_exp_i0 = abund_frame[exp_names[exp_i]].values
            vertex_xs += [vals_exp_i0[cls_i]]
            vertex_ys += [first_vals[cls_i]]
            vertex_colors += [color]
            vertex_sizes += [abund_log_scaled_frame[exp_names[exp_i]].values[cls_i]]
            count_string = (
                f"({vals_exp_i0[cls_i] * 100:.1f} percent)"
                if percent_mode
                else f"({vals_exp_i0[cls_i]} calls)"
            )
            vertex_labels += [
                f"{exp_names[exp_i]} class {labels[cls_i]} {count_string} ni={needle_impact[cls_i] * 100:.1f}%"
            ]

    vertex_source = ColumnDataSource(
        dict(
            label=vertex_labels,
            vertex_xs=vertex_xs,
            vertex_ys=vertex_ys,
            vertex_colors=vertex_colors,
            vertex_sizes=vertex_sizes,
        )
    )

    if percent_mode:
        rng = (1e-6, 0.3)
    else:
        rng = (1, 1e6)

    hover_tool = HoverTool(names=["circles"], tooltips=[("", "@label")])
    tools = [hover_tool, WheelZoomTool(), PanTool(), ResetTool()]
    f = figure(
        x_range=rng,
        y_range=rng,
        x_axis_type="log",
        y_axis_type="log",
        tools=tools,
        title=title,
        x_axis_label=x_label,
        y_axis_label=y_label,
        width=640,
        height=640,
    )
    f.multi_line(line_xs, line_ys, line_color=line_colors, line_width=1)
    f.circle(
        name="circles",
        x="vertex_xs",
        y="vertex_ys",
        fill_color="vertex_colors",
        line_color="vertex_colors",
        size="vertex_sizes",
        source=vertex_source,
    )
    show(f)


# from plaster.run import data_tools
#
# np.set_printoptions(precision=3)
#

# TODO: This is broken due to multiple refactorings!


def abund_abund_runs(
    runs,
    percent_mode=True,
    title="",
    x_label="",
    y_label="",
    highlight_impacted=True,
    omit_needle=False,
):
    needle_class_iz = []
    needle_impact = []

    assert "classify_sigproc" in runs[0]
    peptides = runs[0].classify_sigproc.peptides
    n_classes = peptides.n_real_classes

    if "sim_v1" in runs[0].plaster.plaster_source:
        # probably use protein_of_interest here instead.
        needle_ids = runs[0].plaster.plaster_source.simulation.parameters.prep.needles
        needle_class_iz = [peptides.id_component_to_class_i(id) for id in needle_ids]
        conf = assignment_tools.conf_mat(
            runs[0].classifier.test.true_y, runs[0].classifier.test.pred_y, n_classes
        )
        diag = np.diag(conf)
        rows = np.sum(conf, axis=1)
        self_impact = diag / rows
        # TASK: the following will break with multi-needle
        needle_impact = np.array(
            [
                conf[ci, ni] / np.sum(conf[ci, :])
                for ci in range(n_classes)
                for ni in needle_class_iz
            ]
        )
        # debug(needle_impact)

    labels = [
        peptides.pretty_print_peptide_class_str_by_i(i, terse=True)
        for i in range(n_classes)
    ]

    mode = "percent" if percent_mode else "count"
    default_title = f"Abundance v Abundance, classification {mode} by run"
    if omit_needle:
        default_title += ", background (non-impacted)"
    elif not highlight_impacted:
        default_title += ", background (includes impacted)"

    title = title or default_title
    y_label = y_label or f"Classification {mode}, Run 0"
    x_label = x_label or f"Classification {mode}, Run 1..N"

    ref_df = runs[0].sigproc_df
    exp_names = [(r.run_dir / "..").name for r in runs[1:]]
    dfs = [r.sigproc_df for r in runs[1:]]

    abund_abund_plot(
        needle_class_iz,
        needle_impact,
        labels,
        ref_df,
        exp_names,
        dfs,
        n_classes,
        score_thresh=0.01,
        percent_mode=percent_mode,
        title=title,
        x_label=x_label,
        y_label=y_label,
        highlight_impacted=highlight_impacted,
        omit_needle=omit_needle,
    )


def plot_sigproc_classify_overview(run, top_n_classes=10):
    """
    Overview stats/distribution of signal classification

    Audience:
        Average users

    Goal:
        Allow the user to see:
            if the decoy and real false rates have roughly equal distributions
            if high scores are a good predictor of correctness.

    Plan:
        This is probably not a long-term useful plot as the imposter map
        will probably communicate most of this information in a more useful way.
    """

    bag = run.classify_rf_call_bag()
    pred_counts = np.bincount(bag.pred_pep_iz, minlength=run.prep.n_peps)
    pred_counts_df = pd.DataFrame(
        dict(pep_i=np.arange(run.prep.n_peps), pred_counts=pred_counts)
    )

    pred_counts_df = (
        pred_counts_df.set_index("pep_i")
        .join(run.prep.pros__peps__pepstrs().set_index("pep_i"), how="left")
        .sort_index()
        .reset_index()
    )

    # overview
    md(f"## {bag.n_rows} spots classified")

    cols_to_show = ["pep_i", "pred_counts", "pro_id", "pro_is_decoy", "seqstr"]
    print(pred_counts_df.nlargest(top_n_classes, "pred_counts")[cols_to_show])

    z = ZPlots()
    with z(_cols=2, fill_alpha=0.5, line_alpha=0.05):
        with z(f_title=f"Classification, fraction by index"):
            z.cols(pred_counts, color=z.compare1, legend_label="all classes")

        with z(f_x_range=[0, 1], f_title="Score distribution"):
            z.hist(bag.scores, color=z.compare2)


# PTM-related
# TODO: all of this must be reviewed/rewritten after recent refactors.
# ====================================================================


def _get_prs_for_ptm_positions(run, positions=None):
    assert "ptm" in run
    if not positions:
        if len(run.ptm.proteins_of_interest) == 1:
            ptm_aas = [label[0] for label in run.ptm.ptm_labels]
            poi_seq = next(
                p
                for p in run.prep.proteins
                if p and p.pro_id == run.ptm.proteins_of_interest[0]
            )
            positions = [i for i, aa in enumerate(poi_seq) if aa in ptm_aas]
            if not positions:
                print(
                    "No PTM sites for proteins_of_interest {run.ptm.proteins_of_interest[0]} with labels {ptm_aas}"
                )
                return
        else:
            print("No proteins_of_interest found for PTM run.")
            return
    assert type(positions) is list and type(positions[0]) is int
    positions = sorted(positions)

    df = run.ptm.giant_bag
    scores_col_name = "p4_scores"
    df[scores_col_name] = df.p1_scores * df.p2_scores

    prs_by_position = {}
    for pos in positions:
        true_col_name = f"p4_true_{pos}"
        pred_col_name = f"p4_pred_{pos}"

        # We can't do the PR on all rows, we only want rows corresponding to calls involving
        # global aa position 'pos'.  Those will be (a) all rows for which true_p4_{pos} is
        # non-zero - where p1_true is peptide containing 'pos', AND ADDITIONALLY (b) any rows
        # for which pred_p4_{pos} is non-zero.  SOME of these (b) will be for rows in which
        # the true_p4_{pos} is 0, meaning the p1 call incorrectly predicted the peptide
        # that contains the position of interest (true_p1 doesn't contain the position).
        # (Note that if you were to use ALL rows, you would count as "correct" calls
        # the many rows in which both true and pred will contain 0 because it isn't even
        # the peptide containing this pos, nor was it predicted to be)
        row_mask = np.array((df[true_col_name] > 0) | (df[pred_col_name] > 0))

        debug = False
        if debug:
            ####################################################
            # debug
            ptm_pep = [
                pep
                for pep in run.prep.peptides
                if pep and pep.span_contains(pos, poi_seq)
            ]
            if ptm_pep:
                ptm_pep = ptm_pep[0]
                print(
                    f"\n*** ptm at position {pos} is in peptide {ptm_pep.pro_id(), str(ptm_pep)} ***"
                )
            else:
                print("\n\n**********************************************")
                print(
                    f"*** WTF? no peptide found that contains ptm position {pos} (missing span info? ambiguous?)"
                )
                print("**********************************************\n\n")
                continue

            print(f"  ptm {pos} row_mask len {len(row_mask)} sum {row_mask.sum()}")

            # we expect true_p4_rows to have 1000*2^ptms count
            true_p4_mask = df[true_col_name] > 0
            true_p4_rows = np.argwhere(true_p4_mask).flatten()
            print(f"  we expect true_p4_rows to have 1000*2^ptms count:")
            print(
                f"  true_p4_rows = {true_p4_rows[0]}...{true_p4_rows[-1]} ; {true_p4_mask.sum()} entries"
            )

            # for this position, how many of the p1 calls were correct?
            p1_correct_mask = (
                df.p1_true_pep_iz[true_p4_mask] == df.p1_pred_pep_iz[true_p4_mask]
            )
            print(
                f"  correct p1 calls = {p1_correct_mask.sum()} of {len(p1_correct_mask)}"
            )

            print("call df.pr_curve() ->")
            # end debug
            ####################################################

        p, r, s, _ = df.pr_curve_pep(
            true_col_name=true_col_name,
            pred_col_name=pred_col_name,
            scores_col_name=scores_col_name,
            subset_rows_mask=row_mask,
        )
        prs_by_position[pos] = (p, r, s)
    return prs_by_position


def plot_pr_for_ptms(run, positions=None, merge=True):
    prs_by_position = _get_prs_for_ptm_positions(run, positions)
    with z.Opts(_merge=merge):
        for pos, prs in prs_by_position.items():
            plot_pr_curve(prs)


def _ptm_recall_at_precisions(p, r, precisions):
    recalls = []
    for threshold in precisions:
        p = np.nan_to_num(p, 0)
        arg = utils.np_arg_last_where(p > threshold)
        recalls += [r[arg] if arg is not None else 0]
    return recalls


def _ptm_recall_block(prs_by_position, precisions):
    # Input : dict global_position -> (p,r,s)
    # Output: ndarray 'block' of recalls at various precisions for each position
    #
    # The latter is e.g. output to CSV elsewhere

    n_thresh_precs = len(precisions)
    positions = sorted(prs_by_position.keys())
    assert type(positions[0]) is int
    n_ptm_positions = len(positions)

    block = np.full((n_ptm_positions, n_thresh_precs), np.nan)
    print("\n")
    for i, gpos in enumerate(positions):
        (p, r, s) = prs_by_position[gpos]
        recall_at_pos = _ptm_recall_at_precisions(p, r, precisions)
        block[i, :] = recall_at_pos[:]
    return block


def csv_ptm_precision_recall_report(runs, filename="ptm_recalls.csv", positions=[]):
    raise Exception(
        "DEPRECATED. Thi doesn't seem to be used and is confusingly in percent, not frac"
    )

    # Assemble a CSV
    # Display recall at these precision thresholds
    thresh_precs = np.array([0.99, 0.95, 0.90, 0.80, 0.70])

    row_header_0 = 0
    row_header_1 = 3
    row_header_2 = 4
    row_block_top = 5
    col_header_1 = 0
    col_header_2 = 2
    col_block_left = 3
    col_block_spacer = 1
    grid = np.empty((500, 300), dtype=object)

    recall_blocks = []
    for r in runs:
        prs_by_position = _get_prs_for_ptm_positions(r, positions)
        recall_blocks += [_ptm_recall_block(prs_by_position, thresh_precs)]

    n_ptm, n_precs = recall_blocks[0].shape
    ptm_positions = np.array(sorted(prs_by_position.keys()))
    grid[
        row_block_top : row_block_top + n_ptm, col_header_1 : col_header_1 + 1
    ] = ptm_positions[:, None]
    grid[row_header_1, col_header_1] = "PTM position"
    grid[row_header_2, col_header_2] = "Precision (%):"
    grid[
        row_header_0, col_header_1
    ] = "The recall (%) for each PTM position are given for each precision column (%)"

    n_runs = len(runs)
    for i, run in enumerate(runs):
        recall_block = recall_blocks[i]
        _n_ptm, _n_precs = recall_block.shape
        assert n_ptm == _n_ptm and n_precs == _n_precs
        col = col_block_left + (i * (n_precs + col_block_spacer))
        grid[row_header_1 : row_header_1 + 1, col : col + 1] = run.run_name
        grid[row_header_2 : row_header_2 + 1, col : col + n_precs] = 100 * thresh_precs
        recall_block = np.nan_to_num(recall_block, 0)
        grid[row_block_top : row_block_top + n_ptm, col : col + n_precs] = (
            recall_block * 100.0
        )

    # BEST
    best_col = 1 + col_block_left + (n_runs * (n_precs + col_block_spacer))
    n_thresh_precs = thresh_precs.shape[0]
    all_blocks = np.hstack(recall_blocks)
    assert all_blocks.shape[1] == n_runs * n_thresh_precs
    run_names = np.array([run.run_name for run in runs])
    for i in range(n_thresh_precs):
        stride = i + np.arange(n_runs) * n_thresh_precs
        best_i = np.argmax(all_blocks[:, stride], axis=1)
        best_v = np.max(all_blocks[:, stride], axis=1)
        col = best_col + 2 * i
        grid[row_header_1 : row_header_1 + 1, col : col + 1] = "BEST"
        grid[row_header_2 : row_header_2 + 1, col : col + 1] = 100 * thresh_precs[i]
        grid[row_block_top : row_block_top + n_ptm, col : col + 1] = run_names[
            best_i, None
        ]
        grid[row_block_top : row_block_top + n_ptm, col + 1 : col + 2] = (
            100 * best_v[:, None]
        )

    # EMIT grid to CSV
    print(utils.grid_to_csv(grid, "4.0f"), file=open(filename, "w"))
